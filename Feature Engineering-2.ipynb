{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0118a163-7996-48f2-a951-dab35567c402",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d76f5fb-b7f9-4648-b059-8e80ee41b207",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as min-max normalization, is a data preprocessing technique used to transform numerical features in a dataset to a specific range, typically between 0 and 1. The purpose of Min-Max scaling is to standardize the feature values so that they all fall within the same range, making them directly comparable and preventing features with larger magnitudes from dominating the learning process in machine learning algorithms.\n",
    "\n",
    "The formula for Min-Max scaling for a feature x is as follows:\n",
    "\n",
    "# $X(scaled) = \\frac{X -X(min)}{X(max) - X(min)}$\n",
    "\n",
    "    X(scaled) is the scaled value of the feature \n",
    "    X is the original feature value.\n",
    "    Xmin is the minimum value of the feature X in the dataset.\n",
    "    Xmax is the maximum value of the feature X in the dataset.\n",
    "\n",
    "\n",
    "Min-Max scaling is particularly useful when working with machine learning algorithms that are sensitive to the scale of the input features, such as support vector machines (SVM) or k-nearest neighbors (KNN). By scaling the features to a common range, we ensure that the algorithms consider all features equally and that they perform better and converge faster during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cc6d94-72b3-45eb-a293-dd514493b8c1",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc7980-912a-4e00-acfb-7f5078a36a8c",
   "metadata": {},
   "source": [
    "Unit Vector scaling, also known as vector normalization, is a feature scaling technique used in machine learning to transform numerical features to have a unit length (a length of 1) while preserving their direction. This technique is particularly useful when the direction of the data vectors is more important than their magnitude. Unit Vector scaling is different from Min-Max scaling, which aims to transform features into a specific range, typically between 0 and 1.\n",
    "\n",
    "Here's how Unit Vector scaling works and how it differs from Min-Max scaling:\n",
    "\n",
    "## Unit Vector Scaling (Vector Normalization):\n",
    "\n",
    "1. After normalization, the magnitude (length) of the feature vector becomes 1, while the direction remains the same.\n",
    "2. Unit Vector scaling ensures that each feature contributes equally to the distance calculations in machine learning algorithms.\n",
    "3. It is often used in algorithms like K-Nearest Neighbors (KNN) and in text data analysis.\n",
    "\n",
    "## Min-Max Scaling:\n",
    "\n",
    "\n",
    "1. Min-Max scaling standardizes the feature values within a specified range, making them directly comparable.\n",
    "2. The direction of the data is not preserved, and the magnitudes are adjusted to fit the chosen range (e.g., 0 to 1).\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's illustrate the difference between Unit Vector scaling and Min-Max scaling with an example. Consider a dataset with two features, \"Age\" and \"Income,\" represented as vectors:\n",
    "\n",
    "    Age vector: X(Age)=[25,30,35,40]\n",
    "    Income vector: x(Income)=[50000,60000,70000,80000]\n",
    "\n",
    "## Unit Vector Scaling:\n",
    "\n",
    "Normalize each feature vector to have unit length:\n",
    "X(Age, unit) = [0.447,0.536,0.626,0.715] (approximately)\n",
    "X(Income, unit) = [0.447,0.536,0.626,0.715] (approximately)\n",
    "\n",
    "## Min-Max Scaling:\n",
    "\n",
    "Scale each feature to the range [0, 1]:\n",
    "X(Age, scaled) = [0.0,0.333,0.667,1.0]\n",
    "X(Income, scaled) = [0.0,0.333,0.667,1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae8979e-348c-43e3-8791-60c112f6ef54",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4fe14d-81b0-48b3-a1e2-cca540a5d319",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning and statistics. Its primary purpose is to transform high-dimensional data into a lower-dimensional representation while preserving the most important information or structure in the data. PCA achieves this by finding a new set of orthogonal axes called principal components, which are linear combinations of the original features.\n",
    "\n",
    "Here's how PCA works and how it's used in dimensionality reduction:\n",
    "\n",
    "## PCA Process:\n",
    "\n",
    "1. Standardization: Before applying PCA, it's common practice to standardize the data (mean centering and scaling) to ensure that all features have the same scale.\n",
    "\n",
    "2. Covariance Matrix: PCA calculates the covariance matrix of the standardized data. This matrix quantifies the relationships and variances between pairs of features.\n",
    "\n",
    "3. Eigenvalue Decomposition: PCA then performs eigenvalue decomposition on the covariance matrix. This decomposition yields the eigenvalues and eigenvectors of the matrix.\n",
    "\n",
    "4. Selecting Principal Components: The principal components are the eigenvectors of the covariance matrix. They represent the directions (axes) along which the data varies the most. The eigenvalues correspond to the amount of variance explained by each principal component. PCA orders the principal components by decreasing eigenvalue magnitude, indicating their importance.\n",
    "\n",
    "5. Dimensionality Reduction: To reduce dimensionality, you can select a subset of the top principal components that capture a significant portion of the variance in the data. This reduces the number of features or dimensions while preserving the most critical information.\n",
    "\n",
    "6. Data Transformation: Finally, PCA transforms the original data into the lower-dimensional space defined by the selected principal components.\n",
    "\n",
    "### Example:\n",
    "\n",
    "Let's use a simple example to illustrate PCA's application for dimensionality reduction. Suppose we have a dataset with two features, \"Height\" and \"Weight,\" and we want to reduce it to a single dimension:\n",
    "\n",
    "Original Data:\n",
    "\n",
    "    Data point 1: (Height = 180 cm, Weight = 75 kg)\n",
    "    Data point 2: (Height = 160 cm, Weight = 60 kg)\n",
    "    Data point 3: (Height = 175 cm, Weight = 70 kg)\n",
    "\n",
    "    Standardization: Standardize the data by subtracting the mean and dividing by the standard deviation for each feature.\n",
    "\n",
    "    Covariance Matrix: Calculate the covariance matrix for the standardized data.\n",
    "\n",
    "    Eigenvalue Decomposition: Find the eigenvalues and eigenvectors of the covariance matrix.\n",
    "\n",
    "    Selecting Principal Components: Since we want to reduce to one dimension, we choose the first principal component (associated with the highest eigenvalue).\n",
    "\n",
    "    Data Transformation: Project the original data onto the first principal component to get the reduced representation.\n",
    "\n",
    "The result might look like this:\n",
    "\n",
    "    Data point 1: Reduced to 1D = 1.5\n",
    "    Data point 2: Reduced to 1D = -1.0\n",
    "    Data point 3: Reduced to 1D = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8051af-268f-4bf7-910d-1c540a61fba6",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1166abbf-463b-44f6-ab67-f130d72ec53e",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a technique that can be used for feature extraction, and it plays a crucial role in dimensionality reduction and data compression. Feature extraction is the process of transforming the original features of a dataset into a new set of features that capture the most important information in the data while reducing dimensionality. PCA accomplishes this by finding a set of orthogonal axes (principal components) and using them as the new features.\n",
    "\n",
    "Here's the relationship between PCA and feature extraction, along with an example to illustrate how PCA can be used for feature extraction:\n",
    "\n",
    "## Relationship between PCA and Feature Extraction:\n",
    "\n",
    "    Dimensionality Reduction: PCA is primarily used for dimensionality reduction, which is a form of feature extraction. It reduces the number of features in a dataset while retaining as much of the original data's variance as possible. In this sense, PCA extracts the most critical information from the original features.\n",
    "\n",
    "    Linear Transformation: PCA performs a linear transformation of the data into a new coordinate system defined by the principal components. These principal components are linear combinations of the original features.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let's consider an example with a dataset of images, where each image is represented as a set of pixel values. Each pixel can be considered a feature, and the dimensionality of the dataset can be very high for high-resolution images. PCA can be used for feature extraction in this context:\n",
    "\n",
    "Original Data (Pixel Values):\n",
    "\n",
    "    Image 1: [pixel1, pixel2, ..., pixelN]\n",
    "    Image 2: [pixel1, pixel2, ..., pixelN]\n",
    "    ...\n",
    "Using PCA for Feature Extraction:\n",
    "\n",
    "    Standardization: Standardize the pixel values of all images (mean centering and scaling) to ensure consistent scales across images.\n",
    "\n",
    "    Covariance Matrix: Calculate the covariance matrix for the standardized data. This matrix quantifies the relationships and variances between pixels.\n",
    "\n",
    "    Eigenvalue Decomposition: Perform eigenvalue decomposition on the covariance matrix to obtain the eigenvalues and eigenvectors.\n",
    "\n",
    "    Selecting Principal Components: Choose a subset of the top principal components based on the desired level of dimensionality reduction. These principal components represent directions of maximum variance in the pixel space.\n",
    "\n",
    "    Data Transformation: Transform each image into a new representation using the selected principal components. This reduces the dimensionality of each image while retaining the most critical information. The transformed data represents the images using the most important patterns or features.\n",
    "\n",
    "For example, if you had 1,000 pixel features in each image and you decided to keep only the top 100 principal components, you would reduce the dimensionality of each image to 100 features. These 100 features capture the most significant variations across all images in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350f32bd-2772-4a7f-a28f-b487ae886bf3",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52791d99-e386-4a08-8457-20ccbba4c60a",
   "metadata": {},
   "source": [
    "To preprocess the dataset for building a recommendation system for a food delivery service, you can use Min-Max scaling to standardize the features like price, rating, and delivery time. Min-Max scaling will transform these features to a common range, typically between 0 and 1, ensuring that they have a consistent scale and making them suitable for many machine learning algorithms.\n",
    "\n",
    "Here's how you can use Min-Max scaling to preprocess the data:\n",
    "\n",
    "### Data Preparation:\n",
    "\n",
    "    First, ensure that you have your dataset containing features like \"price,\" \"rating,\" and \"delivery time\" ready for preprocessing.\n",
    "### Determine the Ranges:\n",
    "\n",
    "    Decide on the desired range for the scaled values. The typical range is [0, 1], but you can choose a different range if it better suits your specific project requirements.\n",
    "### Calculate Min and Max Values:\n",
    "\n",
    "    For each feature you want to scale, calculate the minimum (Min) and maximum (Max) values within your dataset. These values will be used in the scaling formula.\n",
    "### Apply Min-Max Scaling:\n",
    "\n",
    "Use the Min-Max scaling formula to transform each feature individually\n",
    "### Repeat for Each Feature:\n",
    "\n",
    "    Apply the Min-Max scaling process separately to each feature in your dataset that you want to scale.\n",
    "### Updated Dataset:\n",
    "\n",
    "    Replace the original feature values with their scaled counterparts. Your dataset will now contain the scaled features in the specified range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "febb07e5-ef63-4d4c-b5cb-6e2f206de8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.30232558, 0.6124031 , 1.        , 1.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        ],\n",
       "       [1.        , 1.        , 0.61832061, 0.85915493]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "price = np.array([10, 20, 30, 40])\n",
    "rating = np.array([3.5, 4.2, 3.8, 4.5])\n",
    "delivery_time = np.array([25, 30, 20, 35])\n",
    "\n",
    "min_max = MinMaxScaler()\n",
    "min_max.fit([price, rating, delivery_time])\n",
    "min_max.transform([price, rating, delivery_time])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e0992-be6d-444f-8fdd-541860a71347",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ed3cb-88b8-406f-a510-27740fff77d1",
   "metadata": {},
   "source": [
    "To reduce the dimensionality of a dataset containing many features, such as company financial data and market trends for predicting stock prices, Principal Component Analysis (PCA) can be a valuable technique. PCA can help you identify the most significant patterns or features in the data while reducing the computational complexity and potential overfitting associated with high-dimensional datasets. Here's how you can use PCA to achieve dimensionality reduction:\n",
    "\n",
    "1. Data Preparation: Ensure that you have your dataset ready, including features like company financial data (e.g., revenue, earnings, debt) and market trends (e.g., stock indices, interest rates).\n",
    "2. Standardization: Standardize the data by subtracting the mean and scaling to unit variance for each feature. This step is crucial because PCA is sensitive to the scale of the features.\n",
    "3. Covariance Matrix: Calculate the covariance matrix of the standardized data. The covariance matrix quantifies the relationships and variances between pairs of features.\n",
    "4. Eigenvalue Decomposition: Perform eigenvalue decomposition on the covariance matrix. This decomposition yields the eigenvalues and eigenvectors of the matrix.\n",
    "5. Selecting Principal Components:\n",
    "    Sort the eigenvalues in descending order. The eigenvalues represent the amount of variance explained by each principal component.\n",
    "    Choose a subset of the top principal components based on the desired level of dimensionality reduction. Typically, you can decide to keep a certain percentage of the total variance (e.g., 95%) or a fixed number of components (e.g., 5 or 10).\n",
    "For example, if you find that the first five principal components explain 90% of the total variance, you might choose to keep these five components.\n",
    "\n",
    "6. Data Transformation: Transform the original data into the lower-dimensional space defined by the selected principal components. This transformation reduces the dimensionality of the dataset while retaining the most critical information.\n",
    "7. Updated Dataset:Replace the original feature values in your dataset with the reduced representation obtained from PCA. Your dataset will now contain the reduced number of features (principal components) that capture the most significant patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf2eddd-4641-4eb1-ac24-9d8f088a847f",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b92612b5-902b-4574-a14f-d3254b3d7049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ],\n",
       "       [-0.57894737],\n",
       "       [-0.05263158],\n",
       "       [ 0.47368421],\n",
       "       [ 1.        ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = np.array([1,5,10,15,20]).reshape(-1,1)\n",
    "min_max = MinMaxScaler((-1,1))\n",
    "min_max.fit(data)\n",
    "min_max.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6881e5e7-00af-493b-b472-11bfa72c3d0f",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dac49b-b8eb-4854-a85c-0204a528c5da",
   "metadata": {},
   "source": [
    "The number of principal components to retain in a PCA-based feature extraction process depends on our specific goals, the characteristics of the dataset, and the amount of variance we want to preserve. Generally, we want to retain enough principal components to capture most of the variance in the data while reducing dimensionality. Here's how we can decide how many principal components to retain:\n",
    "\n",
    "1. Calculate Explained Variance: After performing PCA on our dataset, we will have a set of eigenvalues that represent the variance explained by each principal component. Calculate the cumulative explained variance as we consider an increasing number of components. This cumulative explained variance tells us how much of the total variance in the data is retained as we add more components.\n",
    "\n",
    "2. Set Explained Variance Threshold: Decide on a threshold for the cumulative explained variance that we want to retain. Common choices include retaining 95%, 99%, or any other suitable percentage of the total variance. This threshold is often based on the trade-off between dimensionality reduction and preserving information.\n",
    "\n",
    "3. Choose the Number of Components: Select the number of principal components that, when added up, exceed or reach your chosen threshold of explained variance. These components capture most of the essential information in the data while reducing dimensionality.\n",
    "\n",
    "The choice of how many principal components to retain is somewhat subjective and depends on the specific requirements of our project. Here's a common approach:\n",
    "\n",
    "    If we're using PCA for dimensionality reduction to improve model efficiency and reduce noise, we might start by retaining enough components to explain 95% or 99% of the variance. This ensures that most of the critical patterns in the data are preserved.\n",
    "\n",
    "    If we're using PCA for feature extraction to create a more interpretable representation of the data, we might retain a smaller number of components that still capture a substantial portion of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73838922-9191-4f8a-9185-bd99835ccfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([257.22679642, 497.03230483, 870.72411495, 435.12873329,\n",
       "       804.31007078, 856.14689994, 514.41593609, 268.20763137,\n",
       "       636.88628903, 685.79637266, 564.02101242, 837.05869383,\n",
       "       764.9737696 , 500.78548983, 643.36765369, -36.83040449,\n",
       "       693.91364157, 593.37803918,  71.33650833,  62.31025794,\n",
       "       642.12865815, 114.30998808, 529.0481845 , 983.99800133,\n",
       "       461.26406453, 321.85555974, 685.90348666, 740.75920176,\n",
       "       520.39584236, 686.64768102, 443.2036295 , 671.09581707,\n",
       "       371.19327122, 246.59456357, 403.19722389, 393.7391045 ,\n",
       "       543.20940098, 665.87240408, 408.43904904, 101.20768541,\n",
       "       331.92577851, 688.65615558, 664.34318354, 195.18563127,\n",
       "       713.14741527, 510.22028078, 579.65858875, 706.93872729,\n",
       "       860.17819439, 601.12631695])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(loc = 460, size = 50, scale = 277)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900573e8-612b-4e18-8dd0-a11c7052b727",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
